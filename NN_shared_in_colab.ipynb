{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BimbaBuk/Personal-Projects-/blob/main/NN_shared_in_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ebvpHVqvpVVY"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from google.colab import drive  # Import for Google Drive mounting\n",
        "drive.mount('/content/drive')  # Mount Google Drive\n",
        "drive_dir = '/content/drive/My Drive/NN_Models_Shared/' # Our google Drive directory"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(df, input_columns, target_columns, model_name, MLP_structure, num_epochs, batch_size, learning_rate):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    full_name = model_name + '_MLP_' + MLP_structure + '_epochs_' + str(num_epochs) + '_batch_' + str(batch_size) + '_lr_' + str(learning_rate)\n",
        "    model_dir = drive_dir + full_name + '/'\n",
        "\n",
        "    input_data = df[input_columns].values\n",
        "    target_data = df[target_columns].values\n",
        "    input_data_scaled = StandardScaler().fit_transform(input_data)\n",
        "    target_data_scaled = StandardScaler().fit_transform(target_data)\n",
        "\n",
        "    # Split the data into train and validation sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(input_data_scaled, target_data_scaled, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Convert data to PyTorch tensors\n",
        "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "    y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
        "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "    y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
        "\n",
        "    # Create datasets and dataloaders\n",
        "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "    val_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "    # Initialize model, loss function, and optimizer\n",
        "    model = MLP(input_size=len(input_columns), output_size=len(target_columns)).to(device)\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    writer = SummaryWriter(log_dir=model_dir) # TensorBoard setup\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train() # set the model to training mode\n",
        "        train_loss = 0.0\n",
        "\n",
        "        for inputs, targets in train_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            predictions = model(inputs)\n",
        "            loss = criterion(predictions, targets)\n",
        "\n",
        "            # Backward and optimize\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        # Validation loop\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for inputs, targets in val_loader:\n",
        "                inputs, targets = inputs.to(device), targets.to(device)\n",
        "                predictions = model(inputs)\n",
        "                loss = criterion(predictions, targets)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "        # Calculate average losses\n",
        "        train_loss /= len(train_loader)\n",
        "        val_loss /= len(val_loader)\n",
        "\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}')\n",
        "\n",
        "        # Log losses to TensorBoard\n",
        "        writer.add_scalar('Loss/Train', train_loss, epoch)\n",
        "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
        "\n",
        "        # Add model graph to TensorBoard (only for the first epoch)\n",
        "        if epoch == 0:\n",
        "            dummy_input = torch.randn(1, len(input_columns)).to(device)\n",
        "            writer.add_graph(model, dummy_input)\n",
        "\n",
        "    writer.flush()  # Ensure all data is written to TensorBoard\n",
        "    writer.close()\n",
        "\n",
        "    # Save the model to Google Drive\n",
        "    model_path = model_dir + model_name + '.pth'\n",
        "    torch.save(model.state_dict(), model_path)\n",
        "    print(f\"Model saved to Google Drive as {model_name}\")\n",
        "    return model_dir"
      ],
      "metadata": {
        "id": "QohTSPJgpc--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the MLP model!!!!!!!!!!!!!!!!!!!!!!\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super(MLP, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(input_size, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, output_size)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Load the csv from google drive\n",
        "    csv_name = 'overfit_test.csv' # Choose here csv!!!!!!!!!!!!!!!!!!!!!!\n",
        "    df = pd.read_csv(drive_dir + csv_name)\n",
        "\n",
        "    # Define the input and target columns\n",
        "    input_columns  = ['theta', 'velocity', 'yaw_angle', 'yaw_rate', 'slip_angle', 'front_wheel',\n",
        "                      'back_wheel', 'acceleration', 'angular_acceleration']  # C to K\n",
        "    target_columns = ['delta_x_position', 'delta_y_position', 'delta_theta', 'delta_velocity', 'delta_yaw_angle',\n",
        "                      'delta_yaw_rate', 'delta_slip_angle', 'delta_front_wheel', 'delta_back_wheel']  # L to T\n",
        "\n",
        "    # Train the model: returns the directory where the model saved\n",
        "    model_dir = train_model(df, input_columns, target_columns, model_name=\"overfitcheck\", MLP_structure='256x128', num_epochs=100, batch_size=128, learning_rate=0.001) # Modify!!!!!!!!!!!!!!!!!!!!!!\n",
        "\n",
        "    # Show the training process on TensorBoard\n",
        "    %load_ext tensorboard\n",
        "    %tensorboard --logdir=\"$model_dir\""
      ],
      "metadata": {
        "id": "wOO4Jq6Fwy7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    # Show a saved training process on TensorBoard\n",
        "    model_name = 'overfitcheck_MLP_256x128_epochs_100_batch_128_lr_0.001' # change model name to an existing file in drive!!!!!!!!!!!!!!!!!!!!!!\n",
        "    model_dir = drive_dir + model_name\n",
        "\n",
        "    %load_ext tensorboard\n",
        "    %tensorboard --logdir=\"$model_dir\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    מלכךדגףחכדףלF'S'"
      ],
      "metadata": {
        "id": "0wI9asuwxwbA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}